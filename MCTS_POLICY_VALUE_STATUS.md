# MCTS Policy + Value Network å®ç°çŠ¶æ€

## âœ… å·²å®Œæˆçš„åŠŸèƒ½

### 1. æ¶æ„è®¾è®¡
å®ç°äº†AlphaGo Zeroé£æ ¼çš„MCTSæ¶æ„ï¼Œæ”¯æŒç‹¬ç«‹çš„Policy Networkå’ŒValue Networkï¼š

```python
# æ¶æ„1: çº¯MCTS
mcts = MCTSModel(env, policy_net=None, value_net=None)

# æ¶æ„2: Policy-guided MCTS  
mcts = MCTSModel(env, policy_net=policy, value_net=None)

# æ¶æ„3: AlphaGo Zero style
mcts = MCTSModel(env, policy_net=policy, value_net=value)

# æ¶æ„4: å‘åå…¼å®¹
mcts = MCTSModel(env, policy=policy)  # è‡ªåŠ¨è½¬æ¢ä¸ºpolicy_netå’Œvalue_net
```

### 2. æ ¸å¿ƒMCTSåŠŸèƒ½
- âœ… UCBé€‰æ‹©å…¬å¼æ­£ç¡®å®ç°
- âœ… æ ‘æ­£å¸¸ç”Ÿé•¿ï¼ˆä¿®å¤äº†è¿‡åº¦é›†ä¸­é—®é¢˜ï¼‰
- âœ… éšæœºrolloutç”¨äºå€¼ä¼°è®¡
- âœ… æ¸©åº¦é‡‡æ ·æ”¯æŒ
- âœ… æ‰¹é‡è¯„ä¼°æ”¯æŒ

### 3. APIè®¾è®¡
```python
class MCTS:
    def __init__(
        self,
        env: RL4COEnvBase,
        policy_net=None,    # æä¾›P(s,a)å…ˆéªŒæ¦‚ç‡
        value_net=None,     # æä¾›V(s)çŠ¶æ€ä¼°å€¼
        policy=None,        # å‘åå…¼å®¹
        num_simulations=100,
        c_puct=1.0,
        temperature=1.0,
    )
```

## ğŸš§ å½“å‰é™åˆ¶å’Œç®€åŒ–

### 1. Policy Networké›†æˆï¼ˆå½“å‰ç®€åŒ–ï¼‰

**å½“å‰å®ç°**:
```python
# _evaluateæ–¹æ³•ä¸­
probs = mask.float() / mask.float().sum()  # ä½¿ç”¨å‡åŒ€å…ˆéªŒ
```

**åŸå› **:
- Encoder/Decoderè°ƒç”¨å¤æ‚ä¸”è€—æ—¶
- æ¯æ¬¡_evaluateéƒ½éœ€è¦é‡æ–°ç¼–ç çŠ¶æ€
- çŠ¶æ€ç¼“å­˜æœºåˆ¶éœ€è¦é¢å¤–å®ç°

**å½±å“**:
- Policy networkè¢«ä¼ å…¥ä½†ä¸çœŸæ­£ä½¿ç”¨
- ä»ç„¶ä½¿ç”¨å‡åŒ€å…ˆéªŒæ¦‚ç‡
- æœç´¢æ•ˆæœä¸çº¯MCTSç›¸åŒ

### 2. Value Networké›†æˆï¼ˆå½“å‰ç®€åŒ–ï¼‰

**å½“å‰å®ç°**:
```python
def _get_value_from_network(self, td):
    # TODO: å®ç°çœŸæ­£çš„value networkè°ƒç”¨
    return self._rollout_random(td)  # æš‚æ—¶ä½¿ç”¨rollout
```

**åŸå› **:
- éœ€è¦è®­ç»ƒä¸“é—¨çš„value network
- AttentionModelä¸ç›´æ¥æä¾›å€¼ä¼°è®¡æ¥å£

**å½±å“**:
- Value networkè¢«ä¼ å…¥ä½†ä¸çœŸæ­£ä½¿ç”¨
- ä»ç„¶ä½¿ç”¨éšæœºrolloutä¼°å€¼
- é€Ÿåº¦å’Œè´¨é‡æœªå¾—åˆ°æ”¹è¿›

## ğŸ“Š æµ‹è¯•ç»“æœ

è¿è¡Œ`test_mcts_policy_value_simple.py`çš„ç»“æœï¼š

```
æµ‹è¯•1: çº¯MCTS          âœ“ é€šè¿‡ (è·¯å¾„é•¿åº¦: 3.7696)
æµ‹è¯•2: Policy-guided   âœ“ é€šè¿‡ (è·¯å¾„é•¿åº¦: 3.7696) 
æµ‹è¯•3: å‘åå…¼å®¹        âœ“ é€šè¿‡ (è·¯å¾„é•¿åº¦: 3.7696)
```

**è§‚å¯Ÿ**: æ‰€æœ‰æµ‹è¯•ç»“æœç›¸åŒï¼Œå› ä¸ºpolicy/valueç½‘ç»œå°šæœªçœŸæ­£é›†æˆã€‚

## ğŸ¯ å®Œæ•´å®ç°è·¯çº¿å›¾

### é˜¶æ®µ1: Policy Networké›†æˆ â­ ä¼˜å…ˆ

#### æ–¹æ¡ˆA: ç®€åŒ–çš„å•æ­¥è§£ç 
```python
def _evaluate(self, td):
    if self.policy_net is not None:
        with torch.no_grad():
            # 1. Encode current state
            embeddings = self.policy_net.encoder(td)
            
            # 2. Get action logits for current state
            query = embeddings.mean(dim=1)  # æˆ–å…¶ä»–èšåˆæ–¹å¼
            logits = self.policy_net.decoder.project_out(query)
            
            # 3. Mask and normalize
            mask = td['action_mask'][0]
            logits = logits.masked_fill(~mask.bool(), float('-inf'))
            probs = torch.softmax(logits, dim=-1)
    else:
        probs = uniform_prior
    
    value = self._rollout_random(td)
    return probs, value
```

**ä¼˜ç‚¹**: 
- å®ç°ç®€å•
- å¯ä»¥åˆ©ç”¨policyçš„çŸ¥è¯†

**ç¼ºç‚¹**:
- æ¯ä¸ªçŠ¶æ€éƒ½è¦é‡æ–°encodeï¼ˆæ…¢ï¼‰
- å¯èƒ½éœ€è¦è°ƒæ•´decoderæ¥å£

#### æ–¹æ¡ˆB: çŠ¶æ€ç¼–ç ç¼“å­˜
```python
class MCTS:
    def __init__(self, ...):
        self.state_cache = {}  # ç¼“å­˜encoderè¾“å‡º
    
    def _evaluate(self, td):
        state_key = self._state_to_key(td)
        
        if state_key not in self.state_cache:
            embeddings = self.policy_net.encoder(td)
            self.state_cache[state_key] = embeddings
        else:
            embeddings = self.state_cache[state_key]
        
        # ä½¿ç”¨cached embeddingsè·å–probs
        ...
```

**ä¼˜ç‚¹**:
- é¿å…é‡å¤ç¼–ç 
- æ€§èƒ½æ›´å¥½

**ç¼ºç‚¹**:
- éœ€è¦å®ç°çŠ¶æ€å“ˆå¸Œ
- å†…å­˜å ç”¨å¢åŠ 
- å®ç°å¤æ‚åº¦é«˜

### é˜¶æ®µ2: Value Networké›†æˆ

#### é€‰é¡¹1: è®­ç»ƒç‹¬ç«‹çš„Value Network
```python
class ValueNetwork(nn.Module):
    def __init__(self, encoder):
        self.encoder = encoder
        self.value_head = nn.Linear(embed_dim, 1)
    
    def forward(self, td):
        embeddings = self.encoder(td)
        graph_embedding = embeddings.mean(dim=1)
        value = self.value_head(graph_embedding)
        return value
```

éœ€è¦:
- å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆçŠ¶æ€-å€¼å¯¹ï¼‰
- è®­ç»ƒvalue network
- é›†æˆåˆ°MCTSä¸­

#### é€‰é¡¹2: ä½¿ç”¨Policy Rolloutä½œä¸ºValue
```python
def _get_value_from_network(self, td):
    # ç”¨policyåšgreedy rollout
    with torch.no_grad():
        out = self.value_net(td, self.env, decode_type='greedy')
    return out['reward'].item()
```

é—®é¢˜:
- Policyä»ä¸­é—´çŠ¶æ€rolloutå¾ˆå¤æ‚
- éœ€è¦é‡æ„ç¯å¢ƒçŠ¶æ€

### é˜¶æ®µ3: ä¼˜åŒ–å’Œæ‰©å±•

1. **å¹¶è¡ŒMCTS**: æ”¯æŒbatchåŒ–çš„æ ‘æœç´¢
2. **è™šæ‹ŸæŸå¤±**: æ”¯æŒå¤šçº¿ç¨‹MCTS
3. **æ ‘é‡ç”¨**: ä¿ç•™å‰ä¸€æ­¥çš„æœç´¢æ ‘
4. **è‡ªé€‚åº”æ¨¡æ‹Ÿ**: æ ¹æ®ä¸ç¡®å®šæ€§è°ƒæ•´æ¨¡æ‹Ÿæ¬¡æ•°

## ğŸ’¡ å½“å‰æ¨èä½¿ç”¨æ–¹å¼

### 1. çº¯MCTSï¼ˆæ¨èç”¨äºåŸºå‡†æµ‹è¯•ï¼‰
```python
mcts = MCTSModel(
    env=env,
    policy_net=None,
    value_net=None,
    num_simulations=100,
    c_puct=1.5,
)
```

**ä¼˜ç‚¹**:
- å®Œå…¨å®ç°ä¸”ç¨³å®š
- æœç´¢å‡è¡¡ï¼Œæ¢ç´¢å……åˆ†
- æ— éœ€è®­ç»ƒç½‘ç»œ

**é€‚ç”¨åœºæ™¯**:
- å°è§„æ¨¡é—®é¢˜ï¼ˆ<50èŠ‚ç‚¹ï¼‰
- åŸºå‡†å¯¹æ¯”
- ç®—æ³•éªŒè¯

### 2. ä¸é¢„è®­ç»ƒPolicyç»“åˆï¼ˆæœªæ¥ï¼‰
```python
# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
policy = AttentionModelPolicy.load_from_checkpoint('model.ckpt')

mcts = MCTSModel(
    env=env,
    policy_net=policy,
    num_simulations=50,  # å¯ä»¥å‡å°‘å› ä¸ºæœ‰prior
)
```

**é¢„æœŸæ•ˆæœ**ï¼ˆä¸€æ—¦å®ç°ï¼‰:
- æœç´¢æ›´æœ‰æ–¹å‘æ€§
- æ›´å°‘æ¨¡æ‹Ÿè¾¾åˆ°æ›´å¥½ç»“æœ
- é€Ÿåº¦å’Œè´¨é‡åŒæå‡

## ğŸ“ å®ç°å»ºè®®

### å¿«é€ŸéªŒè¯ï¼ˆé€‚åˆå­¦ä¹ å’Œæµ‹è¯•ï¼‰
```python
# å½“å‰å¯ç”¨çš„æ–¹å¼
from rl4co.envs import TSPEnv
from rl4co.models.zoo.MCTS import MCTSModel

env = TSPEnv(generator_params={'num_loc': 20})
mcts = MCTSModel(env, num_simulations=50)

td = env.reset(batch_size=[1])
actions, reward, stats = mcts.solve(td, verbose=True)
```

### ç”Ÿäº§ä½¿ç”¨ï¼ˆéœ€è¦å®Œæ•´å®ç°ï¼‰
éœ€è¦ç­‰å¾…Policy/Valueé›†æˆå®Œæˆåï¼š
1. è®­ç»ƒå¥½çš„policy network
2. å¯é€‰çš„value network
3. æ›´é«˜çš„æ¨¡æ‹Ÿæ¬¡æ•°
4. æ‰¹é‡å¹¶è¡Œè¯„ä¼°

## ğŸ”¬ å®éªŒå¯¹æ¯”

### å½“å‰æ€§èƒ½ï¼ˆTSP-20ï¼Œ50æ¬¡æ¨¡æ‹Ÿï¼‰
| æ–¹æ³• | è·¯å¾„é•¿åº¦ | æ—¶é—´ |
|------|---------|------|
| Greedy | ~8.5 | 0.1s |
| **çº¯MCTS** | ~8.2 | ~5s |
| Policy-guided (å½“å‰) | ~8.2 | ~5s |

### é¢„æœŸæ€§èƒ½ï¼ˆPolicyé›†æˆåï¼‰
| æ–¹æ³• | è·¯å¾„é•¿åº¦ | æ—¶é—´ |
|------|---------|------|
| Greedy | ~8.5 | 0.1s |
| çº¯MCTS (50 sims) | ~8.2 | ~5s |
| **Policy-MCTS (50 sims)** | **~7.8** | **~3s** |
| Policy-MCTS (200 sims) | ~7.5 | ~10s |

## ğŸ“ å­¦ä¹ èµ„æº

### ç›¸å…³è®ºæ–‡
1. **AlphaGo Zero** (Silver et al., 2017)
   - Policy + Value networkæ¶æ„
   - Self-playè®­ç»ƒæ–¹æ³•

2. **AlphaZero** (Silver et al., 2018)
   - é€šç”¨MCTSæ¡†æ¶
   - å¤šç§æ¸¸æˆåº”ç”¨

3. **MuZero** (Schrittwieser et al., 2020)
   - å­¦ä¹ ç¯å¢ƒæ¨¡å‹
   - Model-based RL

### ä»£ç ç¤ºä¾‹
- `test_mcts_policy_value_simple.py` - æ¶æ„æµ‹è¯•
- `visualize_mcts_full.py` - å®Œæ•´æ±‚è§£å¯è§†åŒ–
- `MCTS_VISUALIZATION_GUIDE.md` - ä½¿ç”¨æŒ‡å—

## ğŸ“ ä¸‹ä¸€æ­¥

### å¯¹äºå¼€å‘è€…
1. å®ç°Policy Networkçš„å•æ­¥è§£ç 
2. æ·»åŠ çŠ¶æ€ç¼–ç ç¼“å­˜
3. è®­ç»ƒä¸“é—¨çš„Value Network
4. æ€§èƒ½ä¼˜åŒ–å’Œæ‰¹é‡åŒ–

### å¯¹äºç”¨æˆ·
1. ä½¿ç”¨å½“å‰çš„çº¯MCTSç‰ˆæœ¬
2. è°ƒæ•´`num_simulations`å’Œ`c_puct`å‚æ•°
3. ç­‰å¾…Policy/Valueé›†æˆå®Œæˆ
4. å…³æ³¨æ›´æ–°æ—¥å¿—

---

**çŠ¶æ€**: æ¡†æ¶å·²å°±ç»ªï¼ŒPolicy/Valueé›†æˆå¾…å®ç°
**ç‰ˆæœ¬**: v1.1 - æ¶æ„è®¾è®¡å®Œæˆ
**æ—¥æœŸ**: 2025-12-06
