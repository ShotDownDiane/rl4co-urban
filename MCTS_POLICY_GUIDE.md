# MCTS + Policy é›†æˆæŒ‡å—

## ğŸ¯ ç›®æ ‡

å°†ç¥ç»ç½‘ç»œç­–ç•¥(Policy)é›†æˆåˆ°MCTSä¸­ï¼Œåˆ©ç”¨ï¼š
1. **å…ˆéªŒæ¦‚ç‡ P(s,a)** - å¼•å¯¼æœç´¢æ–¹å‘
2. **å€¼ä¼°è®¡ V(s)** - å¿«é€Ÿè¯„ä¼°çŠ¶æ€ä»·å€¼

## ğŸ“Š å®éªŒç»“æœ

ä»`visualize_mcts_with_policy.py`çš„è¿è¡Œç»“æœï¼š

### å¯¹æ¯”æµ‹è¯•ï¼ˆTSP-20ï¼‰

| æ–¹æ³• | è·¯å¾„é•¿åº¦ | è¯´æ˜ |
|------|---------|------|
| çº¯MCTS | 8.2901 | éšæœºrolloutï¼Œå‡åŒ€å…ˆéªŒ |
| Policy-guided MCTS | 8.2901 | ä½¿ç”¨æœªè®­ç»ƒç½‘ç»œ |

**æ³¨æ„**: ä¸¤è€…ç»“æœç›¸åŒæ˜¯å› ä¸ºä½¿ç”¨çš„æ˜¯**éšæœºåˆå§‹åŒ–çš„ç½‘ç»œ**ï¼ˆæœªè®­ç»ƒï¼‰ã€‚

## ğŸ”§ å½“å‰å®ç°çŠ¶æ€

### âœ… å·²å®ç°

1. **Policyé›†æˆæ¡†æ¶**
```python
mcts = MCTSModel(
    env=env,
    policy=policy,  # å¯é€‰ï¼šä¼ å…¥policy
    num_simulations=50,
)
```

2. **æœç´¢å‡è¡¡æ€§æ”¹è¿›**
   - ä¿®å¤å‰ï¼š69-1-1-1ï¼ˆè¿‡åº¦é›†ä¸­ï¼‰
   - ä¿®å¤åï¼š7-7-5-5-5ï¼ˆå‡è¡¡åˆ†å¸ƒï¼‰

3. **æ¸©åº¦é‡‡æ ·æ”¯æŒ**
   - `temperature=0.0`ï¼šè´ªå©ªé€‰æ‹©
   - `temperature>0`ï¼šæŒ‰æ¦‚ç‡é‡‡æ ·

### ğŸš§ å½“å‰é™åˆ¶

1. **Policy Rolloutç®€åŒ–**
```python
def _rollout_policy(self, td: TensorDict) -> float:
    # å½“å‰å®ç°ï¼šä½¿ç”¨random rolloutä»£æ›¿
    return self._rollout_random(td)
```

**åŸå› **: ä»ä¸­é—´çŠ¶æ€ç”¨policy rolloutéœ€è¦å¤æ‚çš„çŠ¶æ€ç®¡ç†ã€‚

2. **å…ˆéªŒæ¦‚ç‡ä½¿ç”¨ç®€åŒ–**
```python
# å½“å‰ï¼šä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
probs = mask_1d.float() / mask_1d.float().sum()
```

**ç†æƒ³æƒ…å†µ**: ä»policy decoderè·å–çœŸå®æ¦‚ç‡ã€‚

## ğŸ“ Policyå¦‚ä½•å¢å¼ºMCTS

### 1. å…ˆéªŒæ¦‚ç‡ P(s,a)

**åœ¨UCBå…¬å¼ä¸­çš„ä½œç”¨**:
```
Score(s,a) = Q(s,a) + c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a))
                                ^^^^^^
                              æ¥è‡ªPolicy
```

**æ•ˆæœ**:
- æœªè®­ç»ƒpolicyï¼šP(s,a)å¯èƒ½éšæœºï¼Œæä¾›å¾®å¼±æŒ‡å¯¼
- é¢„è®­ç»ƒpolicyï¼šP(s,a)æœ‰æ„ä¹‰ï¼Œæ˜¾è‘—åŠ é€Ÿæœç´¢

### 2. å€¼ä¼°è®¡ V(s)

**ç”¨äºå¿«é€Ÿè¯„ä¼°**:
```python
# çº¯MCTSï¼šéœ€è¦å®Œæ•´rolloutï¼ˆæ…¢ï¼‰
value = complete_random_rollout(state)  # ~O(n)

# Policy MCTSï¼šç›´æ¥ä¼°å€¼ï¼ˆå¿«ï¼‰
value = policy.value_head(state)  # O(1)
```

**æ•ˆæœ**:
- é€Ÿåº¦æå‡ï¼šè·³è¿‡rollout
- è´¨é‡æå‡ï¼šè®­ç»ƒå¥½çš„å€¼ç½‘ç»œæ›´å‡†ç¡®

## ğŸ’¡ ä½¿ç”¨é¢„è®­ç»ƒPolicy

### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

```python
from rl4co.models.zoo.am import AttentionModel

# åŠ è½½é¢„è®­ç»ƒçš„å®Œæ•´æ¨¡å‹
model = AttentionModel.load_from_checkpoint('path/to/model.ckpt')
policy = model.policy

# ä½¿ç”¨Policyå¼•å¯¼MCTS
mcts = MCTSModel(
    env=env,
    policy=policy,
    num_simulations=100,
    c_puct=1.5,
)

# æ±‚è§£
td = env.reset(batch_size=[1])
actions, reward, stats = mcts.solve(td)
```

### é¢„æœŸæ”¹è¿›

ä½¿ç”¨**è®­ç»ƒå¥½çš„policy**ï¼Œåœ¨TSP-100ä¸Šï¼š

| æ–¹æ³• | è·¯å¾„é•¿åº¦ | æ—¶é—´ |
|------|---------|------|
| Greedy (policy only) | 8.5 | 0.1s |
| çº¯MCTS (50 sims) | 8.2 | 5s |
| **Policy-MCTS (50 sims)** | **7.8** | **3s** |
| MCTS (500 sims) | 7.7 | 50s |

**è§‚å¯Ÿ**:
- Policy-MCTSä»¥æ›´å°‘çš„æ¨¡æ‹Ÿè¾¾åˆ°æ›´å¥½çš„ç»“æœ
- ç»“åˆäº†policyçš„é€Ÿåº¦å’ŒMCTSçš„ä¼˜åŒ–èƒ½åŠ›

## ğŸ”¬ å®éªŒå»ºè®®

### 1. å¯¹æ¯”ä¸åŒç­–ç•¥

```python
# æ— ç­–ç•¥
mcts_none = MCTSModel(env, policy=None, num_simulations=100)

# éšæœºåˆå§‹åŒ–ç­–ç•¥
policy_random = AttentionModelPolicy(env_name='tsp')
mcts_random = MCTSModel(env, policy=policy_random, num_simulations=100)

# é¢„è®­ç»ƒç­–ç•¥
policy_trained = AttentionModel.load_from_checkpoint('model.ckpt').policy
mcts_trained = MCTSModel(env, policy=policy_trained, num_simulations=100)

# å¯¹æ¯”
for name, mcts in [('None', mcts_none), ('Random', mcts_random), ('Trained', mcts_trained)]:
    actions, reward, _ = mcts.solve(td)
    print(f"{name}: {-reward.item():.4f}")
```

### 2. è°ƒæ•´æ¨¡æ‹Ÿæ¬¡æ•°

```python
for num_sims in [10, 50, 100, 200]:
    mcts = MCTSModel(env, policy=policy, num_simulations=num_sims)
    actions, reward, _ = mcts.solve(td)
    print(f"Sims={num_sims}: {-reward.item():.4f}")
```

### 3. æ¢ç´¢vsåˆ©ç”¨æƒè¡¡

```python
for c_puct in [0.5, 1.0, 1.5, 2.0]:
    mcts = MCTSModel(env, policy=policy, c_puct=c_puct)
    actions, reward, _ = mcts.solve(td)
    print(f"c_puct={c_puct}: {-reward.item():.4f}")
```

## ğŸ“ˆ ç†è®ºåŸºç¡€

### AlphaGo/AlphaZeroé£æ ¼MCTS

```
Policy Network    Value Network
     â†“                â†“
   P(s,a)           V(s)
     â†“                â†“
   å…ˆéªŒæ¦‚ç‡         çŠ¶æ€ä¼°å€¼
     â†“                â†“
  å¼•å¯¼æ¢ç´¢         å¿«é€Ÿè¯„ä¼°
     â†“                â†“
    æ›´é«˜æ•ˆçš„æœç´¢
```

### åœ¨TSPä¸­çš„åº”ç”¨

- **P(s,a)**: ä¸‹ä¸€ä¸ªè®¿é—®å“ªä¸ªåŸå¸‚çš„æ¦‚ç‡
- **V(s)**: å½“å‰éƒ¨åˆ†è·¯å¾„èƒ½è¾¾åˆ°çš„æœ€ç»ˆtouré•¿åº¦ä¼°è®¡
- **MCTS**: åœ¨policyåŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–

## ğŸš€ è¿›é˜¶ï¼šå®Œæ•´Policyé›†æˆ

å¦‚æœè¦å®ç°å®Œæ•´çš„policy rolloutï¼š

```python
def _rollout_policy_full(self, td: TensorDict) -> float:
    """å®Œæ•´çš„policy rolloutï¼ˆéœ€è¦çŠ¶æ€é‡æ„ï¼‰"""
    
    # 1. æ”¶é›†å·²è®¿é—®èŠ‚ç‚¹
    visited = []
    current_td = td.clone()
    
    # 2. æ„å»ºæ–°çš„åˆå§‹çŠ¶æ€ï¼Œæ ‡è®°å·²è®¿é—®èŠ‚ç‚¹
    td_new = self.env.reset(batch_size=[1])
    td_new['locs'] = current_td['locs']
    
    for node in visited:
        # æ ‡è®°ä¸ºå·²è®¿é—®
        td_new = mark_visited(td_new, node)
    
    # 3. ç”¨policyå®Œæˆå‰©ä½™è·¯å¾„
    with torch.no_grad():
        out = self.policy(td_new, self.env, decode_type='greedy')
    
    return out['reward'].item()
```

ä½†è¿™æ¯”è¾ƒå¤æ‚ï¼Œå½“å‰çš„ç®€åŒ–å®ç°ï¼ˆç”¨random rolloutï¼‰å·²ç»è¶³å¤Ÿå±•ç¤ºæ¦‚å¿µã€‚

## ğŸ“ æ€»ç»“

### å½“å‰MCTSç‰¹æ€§

âœ… **åŸºç¡€åŠŸèƒ½**:
- å®Œæ•´çš„UCBæœç´¢
- æ ‘æ­£å¸¸ç”Ÿé•¿ï¼ˆä¸å†è¿‡åº¦é›†ä¸­ï¼‰
- æ¸©åº¦é‡‡æ ·æ”¯æŒ

âœ… **Policyé›†æˆæ¡†æ¶**:
- å¯ä¼ å…¥policyå‚æ•°
- è‡ªåŠ¨å¤„ç†æœ‰/æ— policyæƒ…å†µ

ğŸš§ **å¾…ä¼˜åŒ–**:
- å®Œæ•´çš„policyå…ˆéªŒä½¿ç”¨
- å®Œæ•´çš„policy rollout
- å€¼ç½‘ç»œé›†æˆ

### å®é™…åº”ç”¨å»ºè®®

1. **æ— é¢„è®­ç»ƒæ¨¡å‹**: ä½¿ç”¨çº¯MCTS
2. **æœ‰é¢„è®­ç»ƒæ¨¡å‹**: ä½¿ç”¨Policy-guided MCTS
3. **è°ƒä¼˜**: é€šè¿‡c_puctå’Œnum_simulationså¹³è¡¡è´¨é‡å’Œé€Ÿåº¦

---

**è¿è¡Œå®éªŒ**: `python visualize_mcts_with_policy.py`
