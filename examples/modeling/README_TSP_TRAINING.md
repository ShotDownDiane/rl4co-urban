# TSPè®­ç»ƒæŒ‡å— - AttentionModel

## ğŸ“‹ æ¦‚è¿°

TSP (Traveling Salesman Problem) æ˜¯ç»„åˆä¼˜åŒ–ä¸­æœ€ç»å…¸çš„é—®é¢˜ï¼Œä¹Ÿæ˜¯AttentionModelæœ€åˆè®¾è®¡çš„ç›®æ ‡é—®é¢˜ã€‚æœ¬è„šæœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨RL4COè®­ç»ƒAttentionModelæ¥è§£å†³TSPã€‚

**é—®é¢˜æè¿°ï¼š** ç»™å®šnä¸ªåŸå¸‚ï¼Œæ‰¾åˆ°è®¿é—®æ‰€æœ‰åŸå¸‚æ°å¥½ä¸€æ¬¡å¹¶è¿”å›èµ·ç‚¹çš„æœ€çŸ­è·¯å¾„ã€‚

## ğŸ¯ ç‰¹æ€§

- âœ… **å¤šç§è§„æ¨¡**: æ”¯æŒ20åŸå¸‚ã€50åŸå¸‚ç­‰ä¸åŒè§„æ¨¡
- âœ… **å¤šç§è§£ç ç­–ç•¥**: Greedyã€Samplingã€Beam Search
- âœ… **é«˜æ•ˆè®­ç»ƒ**: ä½¿ç”¨REINFORCE + Rollout Baseline
- âœ… **å®Œæ•´è¯„ä¼°**: å¯¹æ¯”ä¸åŒè§£ç ç­–ç•¥çš„æ€§èƒ½
- âœ… **æ··åˆç²¾åº¦**: æ”¯æŒ16-bitè®­ç»ƒåŠ é€Ÿ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€è®­ç»ƒ (20åŸå¸‚)

```bash
cd examples/modeling
python test_tsp.py --mode simple
```

**é…ç½®ï¼š**
- åŸå¸‚æ•°é‡: 20
- è®­ç»ƒå®ä¾‹: 100,000
- Batch size: 512
- è®­ç»ƒè½®æ•°: 100 epochs
- é¢„æœŸè®­ç»ƒæ—¶é—´: ~2-3å°æ—¶ (å•GPU)

### å¤§è§„æ¨¡è®­ç»ƒ (50åŸå¸‚)

```bash
python test_tsp.py --mode larger
```

**é…ç½®ï¼š**
- åŸå¸‚æ•°é‡: 50
- è®­ç»ƒå®ä¾‹: 100,000
- Batch size: 256 (æ›´å¤§é—®é¢˜éœ€è¦æ›´å°batch)
- è®­ç»ƒè½®æ•°: 100 epochs
- é¢„æœŸè®­ç»ƒæ—¶é—´: ~4-6å°æ—¶ (å•GPU)

### Beam Searchè®­ç»ƒ

```bash
python test_tsp.py --mode beam
```

**ç‰¹ç‚¹ï¼š**
- è®­ç»ƒæ—¶ä½¿ç”¨sampling
- æµ‹è¯•æ—¶ä½¿ç”¨beam search (width=5)
- é€šå¸¸èƒ½è·å¾—æ›´å¥½çš„è§£è´¨é‡

### è¯„ä¼°å·²è®­ç»ƒæ¨¡å‹

```bash
python test_tsp.py --mode evaluate
```

**å¯¹æ¯”ç­–ç•¥ï¼š**
- Greedy decoding
- Sampling
- Beam search (width=5)
- Beam search (width=10)

## ğŸ“Š é¢„æœŸæ€§èƒ½

### TSP-20 (20åŸå¸‚)

| æ–¹æ³• | å¹³å‡Touré•¿åº¦ | è®­ç»ƒæ—¶é—´ | æ¨ç†é€Ÿåº¦ |
|------|-------------|---------|---------|
| Greedy | ~3.8-3.9 | - | ~1000 inst/s |
| Sampling | ~3.85-3.95 | - | ~800 inst/s |
| Beam (w=5) | ~3.75-3.85 | - | ~200 inst/s |
| Beam (w=10) | ~3.73-3.83 | - | ~100 inst/s |

*å‚è€ƒï¼šæœ€ä¼˜è§£çº¦ä¸º3.7-3.8*

### TSP-50 (50åŸå¸‚)

| æ–¹æ³• | å¹³å‡Touré•¿åº¦ | è®­ç»ƒæ—¶é—´ | æ¨ç†é€Ÿåº¦ |
|------|-------------|---------|---------|
| Greedy | ~5.7-5.8 | - | ~500 inst/s |
| Sampling | ~5.75-5.85 | - | ~400 inst/s |
| Beam (w=5) | ~5.65-5.75 | - | ~100 inst/s |

*å‚è€ƒï¼šæœ€ä¼˜è§£çº¦ä¸º5.6-5.7*

## ğŸ—ï¸ æ¶æ„è¯¦è§£

### æ¨¡å‹ç»“æ„

```
Input: åŸå¸‚åæ ‡ [batch_size, num_cities, 2]
  â†“
Encoder (Multi-head Attention)
  - 3 layers
  - 8 attention heads
  - 128 embedding dimension
  â†“
Node Embeddings [batch_size, num_cities, 128]
  â†“
Decoder (Autoregressive)
  - Context embedding (å½“å‰çŠ¶æ€)
  - Pointer network (é€‰æ‹©ä¸‹ä¸€ä¸ªåŸå¸‚)
  â†“
Output: Tour [batch_size, num_cities]
```

### è®­ç»ƒç®—æ³•

**REINFORCE with Rollout Baseline**

```python
# è®­ç»ƒè¿‡ç¨‹
for batch in dataloader:
    # 1. Sampling: ä»ç­–ç•¥é‡‡æ ·è·å¾—tour
    Ï€_Î¸, tour_sample = policy.sample(batch)
    reward_sample = -tour_length(tour_sample)
    
    # 2. Baseline: è´ªå¿ƒè§£ç è·å¾—baseline
    tour_greedy = policy.greedy(batch)
    reward_baseline = -tour_length(tour_greedy)
    
    # 3. Advantage: è®¡ç®—ä¼˜åŠ¿å‡½æ•°
    advantage = reward_sample - reward_baseline
    
    # 4. Policy Gradient: æ›´æ–°ç­–ç•¥
    loss = -log(Ï€_Î¸) * advantage
    loss.backward()
```

## ğŸ“ è¾“å‡ºæ–‡ä»¶

### Checkpoints

```
checkpoints/tsp/
â”œâ”€â”€ tsp20-epoch=00-val_reward=3.85.ckpt
â”œâ”€â”€ tsp20-epoch=50-val_reward=3.82.ckpt
â””â”€â”€ tsp20-epoch=99-val_reward=3.79.ckpt  # Best model
```

### TensorBoard Logs

```
logs/tsp/
â”œâ”€â”€ version_0/
â”‚   â”œâ”€â”€ events.out.tfevents...
â”‚   â””â”€â”€ hparams.yaml
â””â”€â”€ ...
```

**æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹ï¼š**
```bash
tensorboard --logdir logs/tsp/
```

## ğŸ”§ è‡ªå®šä¹‰è®­ç»ƒ

### ä¿®æ”¹è¶…å‚æ•°

```python
# åœ¨ test_tsp.py ä¸­ä¿®æ”¹

# ç¯å¢ƒå‚æ•°
env = TSPEnv(generator_params={
    "num_loc": 100,  # æ”¹ä¸º100åŸå¸‚
})

# æ¨¡å‹å‚æ•°
policy = AttentionModelPolicy(
    env_name=env.name,
    embed_dim=256,           # å¢å¤§embeddingç»´åº¦
    num_encoder_layers=6,    # å¢åŠ å±‚æ•°
    num_heads=16,            # å¢åŠ attention heads
)

# è®­ç»ƒå‚æ•°
model = AttentionModel(
    env,
    policy=policy,
    batch_size=128,          # è°ƒæ•´batch size
    optimizer_kwargs={"lr": 5e-5},  # è°ƒæ•´å­¦ä¹ ç‡
)

# Trainerå‚æ•°
trainer = pl.Trainer(
    max_epochs=200,          # å¢åŠ è®­ç»ƒè½®æ•°
    precision="32",          # ä½¿ç”¨å…¨ç²¾åº¦
)
```

### ä½¿ç”¨é¢„ç”Ÿæˆæ•°æ®

```python
# 1. é¢„ç”Ÿæˆæ•°æ®é›†
from rl4co.data.utils import save_tensordict_to_npz

td_train = env.generate_data(batch_size=[100000])
save_tensordict_to_npz(td_train, "data/tsp20_train.npz")

# 2. åœ¨è®­ç»ƒæ—¶åŠ è½½
from rl4co.data.utils import load_npz_to_tensordict

td_train = load_npz_to_tensordict("data/tsp20_train.npz")
```

## ğŸ“ˆ è®­ç»ƒæ›²çº¿è§£è¯»

### å…³é”®æŒ‡æ ‡

1. **train/reward**: è®­ç»ƒé›†å¹³å‡reward (è¶Šé«˜è¶Šå¥½ï¼Œè´Ÿtouré•¿åº¦)
2. **val/reward**: éªŒè¯é›†å¹³å‡reward
3. **train/loss**: Policy gradient loss
4. **val/loss**: éªŒè¯é›†loss

### æ­£å¸¸è®­ç»ƒæ›²çº¿

```
Epoch  Train Reward  Val Reward    Val Tour Length
----------------------------------------------------
0      -4.2          -4.3          4.3
10     -4.0          -4.1          4.1
20     -3.9          -4.0          4.0
50     -3.85         -3.88         3.88
100    -3.79         -3.82         3.82  â† æ”¶æ•›
```

### è¯Šæ–­é—®é¢˜

**1. Lossä¸ä¸‹é™**
- æ£€æŸ¥å­¦ä¹ ç‡ (å¯èƒ½å¤ªå¤§æˆ–å¤ªå°)
- æ£€æŸ¥gradient clipping
- å°è¯•warm-upç­–ç•¥

**2. Validationæ€§èƒ½å·®**
- è¿‡æ‹Ÿåˆï¼šå‡å°æ¨¡å‹å¤æ‚åº¦ï¼Œå¢åŠ è®­ç»ƒæ•°æ®
- æ¬ æ‹Ÿåˆï¼šå¢å¤§æ¨¡å‹å®¹é‡ï¼Œè®­ç»ƒæ›´å¤šè½®

**3. è®­ç»ƒä¸ç¨³å®š**
- å‡å°å­¦ä¹ ç‡
- å¢åŠ gradient clipping
- ä½¿ç”¨æ›´å¤§çš„batch size

## ğŸ“ è¿›é˜¶æŠ€å·§

### 1. å­¦ä¹ ç‡è°ƒåº¦

```python
from torch.optim.lr_scheduler import CosineAnnealingLR

model = AttentionModel(
    env, policy,
    optimizer_kwargs={
        "lr": 1e-4,
        "lr_scheduler": "CosineAnnealingLR",
        "lr_scheduler_kwargs": {"T_max": 100},
    }
)
```

### 2. å¤šGPUè®­ç»ƒ

```python
trainer = pl.Trainer(
    max_epochs=100,
    accelerator="gpu",
    devices=4,              # ä½¿ç”¨4ä¸ªGPU
    strategy="ddp",         # Distributed Data Parallel
)
```

### 3. Early Stopping

```python
from lightning.pytorch.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor="val/reward",
    patience=10,
    mode="max",
)

trainer = pl.Trainer(
    callbacks=[checkpoint_callback, early_stop]
)
```

### 4. å¢å¼ºæ•°æ®åˆ†å¸ƒ

```python
# æ··åˆä¸åŒå°ºåº¦çš„åŸå¸‚åˆ†å¸ƒ
env = TSPEnv(generator_params={
    "num_loc": 20,
    "min_loc": 0.0,
    "max_loc": 1.0,
    # å¯ä»¥æ·»åŠ èšç±»åˆ†å¸ƒç­‰
})
```

## ğŸ” æ€§èƒ½ä¼˜åŒ–

### æ¨ç†åŠ é€Ÿ

```python
# 1. ä½¿ç”¨JITç¼–è¯‘
model = torch.jit.script(model)

# 2. ä½¿ç”¨æ‰¹é‡æ¨ç†
batch_size = 1024  # æ›´å¤§çš„æ‰¹é‡

# 3. ä½¿ç”¨greedyè§£ç ï¼ˆæœ€å¿«ï¼‰
model.policy.decode_type = "greedy"

# 4. åŠç²¾åº¦æ¨ç†
model = model.half()
```

### å†…å­˜ä¼˜åŒ–

```python
# 1. å‡å°batch size
batch_size = 256

# 2. æ¢¯åº¦ç´¯ç§¯
trainer = pl.Trainer(
    accumulate_grad_batches=4  # ç´¯ç§¯4ä¸ªbatch
)

# 3. æ··åˆç²¾åº¦
trainer = pl.Trainer(
    precision="16-mixed"
)
```

## ğŸ“š å‚è€ƒèµ„æ–™

### è®ºæ–‡
- **Attention, Learn to Solve Routing Problems!** (Kool et al., 2019)
  - [Paper](https://arxiv.org/abs/1803.08475)
  - [Original Code](https://github.com/wouterkool/attention-learn-to-route)

### RL4COæ–‡æ¡£
- [RL4CO GitHub](https://github.com/ai4co/rl4co)
- [Documentation](https://rl4co.readthedocs.io/)

## â“ å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ
**A**: åœ¨å•ä¸ªV100 GPUä¸Šï¼š
- TSP-20: ~2-3å°æ—¶ (100 epochs)
- TSP-50: ~4-6å°æ—¶ (100 epochs)
- TSP-100: ~10-15å°æ—¶ (100 epochs)

### Q2: éœ€è¦å¤šå°‘GPUå†…å­˜ï¼Ÿ
**A**: 
- TSP-20 (batch=512): ~6GB
- TSP-50 (batch=256): ~8GB
- TSP-100 (batch=128): ~12GB

### Q3: å¦‚ä½•æé«˜è§£çš„è´¨é‡ï¼Ÿ
**A**: 
1. ä½¿ç”¨beam search (width=10-20)
2. è®­ç»ƒæ›´é•¿æ—¶é—´ (200+ epochs)
3. å¢å¤§æ¨¡å‹å®¹é‡ (æ›´å¤šlayers/heads)
4. ä½¿ç”¨ensemble (å¤šä¸ªæ¨¡å‹æŠ•ç¥¨)

### Q4: èƒ½å¦è¿ç§»åˆ°å…¶ä»–è§„æ¨¡ï¼Ÿ
**A**: éƒ¨åˆ†å¯ä»¥ï¼š
- TSP-20è®­ç»ƒçš„æ¨¡å‹å¯ä»¥æ¨å¹¿åˆ°TSP-30
- ä½†å¯¹TSP-100æ•ˆæœä¼šä¸‹é™
- å»ºè®®é’ˆå¯¹ç›®æ ‡è§„æ¨¡å•ç‹¬è®­ç»ƒ

### Q5: ä¸ä¼ ç»Ÿå¯å‘å¼ç®—æ³•æ¯”è¾ƒå¦‚ä½•ï¼Ÿ
**A**: 
- **é€Ÿåº¦**: ç¥ç»ç½‘ç»œå¿«10-100å€
- **è´¨é‡**: æ¥è¿‘LKHç­‰å¯å‘å¼ç®—æ³• (å·®è·<5%)
- **æ³›åŒ–**: å¯ä»¥å¤„ç†ä¸åŒåˆ†å¸ƒçš„å®ä¾‹

## ğŸ‰ æ€»ç»“

TSPæ˜¯å­¦ä¹ ç»„åˆä¼˜åŒ–ç¥ç»ç½‘ç»œæ–¹æ³•çš„æœ€ä½³å…¥é—¨é—®é¢˜ï¼š
- âœ… é—®é¢˜ç®€å•æ˜ç¡®
- âœ… AttentionModelä¸“ä¸ºTSPè®¾è®¡
- âœ… è®­ç»ƒå¿«é€Ÿç¨³å®š
- âœ… ç»“æœå®¹æ˜“å¯è§†åŒ–
- âœ… æ€§èƒ½ä¼˜ç§€å¯é 

**ä¸‹ä¸€æ­¥ï¼š**
1. æŒæ¡TSPåï¼Œå°è¯•CVRP (å¸¦å®¹é‡çº¦æŸ)
2. æ¢ç´¢MCLPã€FLPç­‰å›¾ä¼˜åŒ–é—®é¢˜
3. å­¦ä¹ æ”¹è¿›æ–¹æ³• (POMO, Sym-NCOç­‰)

---

**Happy Training! ğŸš€**
