# FLP Training Guide

å®Œæ•´çš„FLP (Facility Location Problem) è®­ç»ƒç¤ºä¾‹ï¼Œå‚è€ƒTSPçš„å®ç°ã€‚

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

- âœ… **Simple Mode**: å¿«é€Ÿè®­ç»ƒç¤ºä¾‹ï¼ˆ10 epochsï¼‰
- âœ… **Advanced Mode**: å®Œæ•´è®­ç»ƒé…ç½®ï¼ˆ50 epochsï¼Œbeam searchï¼‰
- âœ… **From Checkpoint**: ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ
- âœ… **Evaluate Mode**: è¯„ä¼°å·²è®­ç»ƒæ¨¡å‹

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. ç®€å•è®­ç»ƒï¼ˆæ¨èå…¥é—¨ï¼‰

```bash
cd examples/modeling
python test_flp.py --mode simple
```

**é…ç½®ï¼š**
- é—®é¢˜è§„æ¨¡ï¼š50ä¸ªå€™é€‰ç‚¹ï¼Œé€‰æ‹©5ä¸ªè®¾æ–½
- è®­ç»ƒæ•°æ®ï¼š10,000ä¸ªå®ä¾‹
- è®­ç»ƒè½®æ•°ï¼š10 epochs
- è§£ç ç­–ç•¥ï¼š
  - è®­ç»ƒæ—¶ï¼šsamplingï¼ˆä»åˆ†å¸ƒä¸­é‡‡æ ·ï¼‰
  - éªŒè¯æ—¶ï¼šgreedyï¼ˆè´ªå¿ƒï¼‰
  - æµ‹è¯•æ—¶ï¼šgreedy

**è¾“å‡ºï¼š**
- Checkpoints: `checkpoints/flp/`
- TensorBoard logs: `logs/flp/`

### 2. é«˜çº§è®­ç»ƒ

```bash
python test_flp.py --mode advanced
```

**é…ç½®ï¼š**
- é—®é¢˜è§„æ¨¡ï¼š100ä¸ªå€™é€‰ç‚¹ï¼Œé€‰æ‹©10ä¸ªè®¾æ–½ï¼ˆæ›´å¤§ï¼‰
- è®­ç»ƒæ•°æ®ï¼š50,000ä¸ªå®ä¾‹
- è®­ç»ƒè½®æ•°ï¼š50 epochs
- è§£ç ç­–ç•¥ï¼š
  - è®­ç»ƒæ—¶ï¼šsampling
  - éªŒè¯æ—¶ï¼šgreedy
  - æµ‹è¯•æ—¶ï¼šbeam search (width=5) â­ æ›´é«˜è´¨é‡

### 3. ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ

```bash
python test_flp.py --mode from_ckpt
```

éœ€è¦å…ˆæœ‰checkpointæ–‡ä»¶ï¼ˆé€šè¿‡simpleæˆ–advancedæ¨¡å¼ç”Ÿæˆï¼‰ã€‚

### 4. è¯„ä¼°æ¨¡å‹

```bash
python test_flp.py --mode evaluate
```

å¯¹æ¯”ä¸åŒè§£ç ç­–ç•¥çš„æ•ˆæœï¼š
- Greedy decoding
- Sampling (10æ¬¡é‡‡æ ·å–æœ€å¥½)

## ğŸ“Š ç›‘æ§è®­ç»ƒ

### TensorBoard

```bash
tensorboard --logdir logs/flp/
```

ç„¶ååœ¨æµè§ˆå™¨æ‰“å¼€ `http://localhost:6006`

**å¯ä»¥çœ‹åˆ°ï¼š**
- Training loss
- Validation reward
- Learning rate
- Gradient norms

## ğŸ“ æ ¸å¿ƒæ¦‚å¿µ

### 1. Environmentï¼ˆç¯å¢ƒï¼‰

```python
env = FLPEnv(generator_params={
    "num_loc": 50,      # å€™é€‰è®¾æ–½ä½ç½®æ•°é‡
    "to_choose": 5,     # éœ€è¦é€‰æ‹©çš„è®¾æ–½æ•°é‡
})
```

### 2. Policyï¼ˆç­–ç•¥ç½‘ç»œï¼‰

```python
policy = AttentionModelPolicy(
    env_name=env.name,
    embed_dim=128,           # åµŒå…¥ç»´åº¦
    num_encoder_layers=3,    # ç¼–ç å™¨å±‚æ•°
    num_heads=8,             # æ³¨æ„åŠ›å¤´æ•°
)
```

**æ¶æ„ï¼š** Encoder-Decoder with Attention
- **Encoder**: å¤„ç†æ‰€æœ‰å€™é€‰ä½ç½®ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡è¡¨ç¤º
- **Decoder**: è‡ªå›å½’åœ°é€‰æ‹©è®¾æ–½ï¼Œæ¯æ¬¡é€‰æ‹©ä¸€ä¸ª

### 3. Modelï¼ˆRLç®—æ³•ï¼‰

```python
model = AttentionModel(
    env,
    policy=policy,
    baseline="rollout",     # åŸºçº¿ï¼šrolloutï¼ˆè´ªå¿ƒç­–ç•¥çš„å¥–åŠ±ä½œä¸ºåŸºçº¿ï¼‰
    batch_size=512,         # è®­ç»ƒæ‰¹æ¬¡å¤§å°
    optimizer_kwargs={"lr": 1e-4},  # å­¦ä¹ ç‡
)
```

**ç®—æ³•ï¼š** REINFORCE with Baseline
- ç›®æ ‡ï¼šæœ€å¤§åŒ–æœŸæœ›å¥–åŠ±
- åŸºçº¿ï¼šå‡å°‘æ–¹å·®ï¼ŒåŠ é€Ÿè®­ç»ƒ

### 4. Decoding Strategiesï¼ˆè§£ç ç­–ç•¥ï¼‰

| ç­–ç•¥ | ç‰¹ç‚¹ | ç”¨é€” |
|------|------|------|
| **Sampling** | ä»åˆ†å¸ƒä¸­é‡‡æ · | è®­ç»ƒæ—¶æ¢ç´¢ |
| **Greedy** | æ€»æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œ | éªŒè¯/å¿«é€Ÿæ¨ç† |
| **Beam Search** | ä¿ç•™top-kä¸ªå€™é€‰åºåˆ— | æµ‹è¯•/é«˜è´¨é‡è§£ |

## ğŸ“ˆ é¢„æœŸç»“æœ

### è®­ç»ƒæ›²çº¿

- **Epoch 1-3**: å¿«é€Ÿä¸‹é™ï¼ˆå­¦ä¹ åŸºç¡€ç­–ç•¥ï¼‰
- **Epoch 4-7**: å¹³ç¨³æ”¹è¿›
- **Epoch 8-10**: æ”¶æ•›

### FLP Reward

FLPçš„rewardæ˜¯**è´Ÿçš„æ€»è·ç¦»**ï¼ˆè¶Šå¤§è¶Šå¥½ï¼Œå³è·ç¦»è¶Šå°ï¼‰ï¼š
```
Reward = -sum(min(distance[i, selected_facilities]))
```

**å…¸å‹å€¼ï¼š**
- éšæœºç­–ç•¥ï¼š~-5.0 åˆ° -6.0
- è®­ç»ƒåï¼š~-3.0 åˆ° -4.0
- æ¥è¿‘æœ€ä¼˜ï¼š~-2.5

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

### ä¿®æ”¹é—®é¢˜è§„æ¨¡

```python
env = FLPEnv(generator_params={
    "num_loc": 100,      # å¢åŠ åˆ°100
    "to_choose": 20,     # é€‰æ‹©20ä¸ª
})
```

### ä¿®æ”¹ç½‘ç»œæ¶æ„

```python
policy = AttentionModelPolicy(
    env_name=env.name,
    embed_dim=256,           # å¢åŠ åµŒå…¥ç»´åº¦
    num_encoder_layers=6,    # æ›´æ·±çš„ç¼–ç å™¨
    num_heads=16,            # æ›´å¤šæ³¨æ„åŠ›å¤´
    normalization="batch",   # æ‰¹å½’ä¸€åŒ–
    feed_forward_hidden=512, # FFå±‚ç»´åº¦
)
```

### ä¿®æ”¹è®­ç»ƒå‚æ•°

```python
model = AttentionModel(
    env,
    baseline="rollout",
    batch_size=1024,            # æ›´å¤§æ‰¹æ¬¡
    val_batch_size=128,
    train_data_size=100_000,    # æ›´å¤šæ•°æ®
    optimizer_kwargs={
        "lr": 1e-4,
        "weight_decay": 1e-6,   # æƒé‡è¡°å‡
    },
    lr_scheduler={              # å­¦ä¹ ç‡è°ƒåº¦
        "type": "StepLR",
        "step_size": 10,
        "gamma": 0.96,
    },
)
```

## ğŸ’¡ Tips

### 1. æ˜¾å­˜ä¸è¶³ï¼Ÿ

- å‡å°‘ `batch_size`
- å‡å°‘ `embed_dim`
- å‡å°‘ `num_encoder_layers`

### 2. è®­ç»ƒå¤ªæ…¢ï¼Ÿ

- å¢åŠ  `batch_size`ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
- å‡å°‘ `train_data_size`ï¼ˆå¿«é€Ÿå®éªŒï¼‰
- ä½¿ç”¨æ›´å°çš„é—®é¢˜è§„æ¨¡

### 3. æ¨¡å‹ä¸æ”¶æ•›ï¼Ÿ

- æ£€æŸ¥å­¦ä¹ ç‡ï¼ˆå°è¯• 1e-5 æˆ– 1e-3ï¼‰
- å¢åŠ è®­ç»ƒè½®æ•°
- æ£€æŸ¥æ¢¯åº¦è£å‰ªï¼ˆ`gradient_clip_val`ï¼‰
- å°è¯•ä¸åŒçš„baselineï¼ˆ`shared`, `exponential`ï¼‰

### 4. æƒ³è¦æ›´å¥½çš„è§£ï¼Ÿ

- è®­ç»ƒæ›´å¤šè½®æ•°
- ä½¿ç”¨beam searchè¿›è¡Œæ¨ç†
- ä½¿ç”¨samplingç­–ç•¥å¤šæ¬¡é‡‡æ ·å–æœ€ä¼˜

## ğŸ“š ä»£ç ç»“æ„

```python
# 1. åˆ›å»ºç¯å¢ƒ
env = FLPEnv(...)

# 2. åˆ›å»ºç­–ç•¥ç½‘ç»œ
policy = AttentionModelPolicy(...)

# 3. åˆ›å»ºRLæ¨¡å‹
model = AttentionModel(env, policy, ...)

# 4. åˆ›å»ºè®­ç»ƒå™¨
trainer = pl.Trainer(...)

# 5. è®­ç»ƒ
trainer.fit(model)

# 6. æµ‹è¯•
trainer.test(model)
```

## ğŸ” å¯¹æ¯”ä¸åŒæ–¹æ³•

| æ–¹æ³• | é€Ÿåº¦ | è´¨é‡ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| Random | âš¡âš¡âš¡ | â­ | Baseline |
| Greedy Heuristic | âš¡âš¡ | â­â­ | å¿«é€Ÿè¿‘ä¼¼ |
| RL (Greedy) | âš¡âš¡ | â­â­â­ | å®æ—¶æ¨ç† |
| RL (Sampling x10) | âš¡ | â­â­â­â­ | ç¦»çº¿ä¼˜åŒ– |
| RL (Beam Search) | âš¡ | â­â­â­â­â­ | é«˜è´¨é‡è§£ |
| Exact Solver | ğŸŒ | â­â­â­â­â­ | å°è§„æ¨¡æœ€ä¼˜ |

## ğŸ¯ ä¸‹ä¸€æ­¥

1. **å°è¯•ä¸åŒé—®é¢˜è§„æ¨¡**: ä»å°åˆ°å¤§é€æ­¥å¢åŠ 
2. **å¯¹æ¯”ä¸åŒbaseline**: `rollout`, `shared`, `exponential`
3. **å°è¯•å…¶ä»–æ¨¡å‹**: POMO, SymNCO
4. **è¿ç§»åˆ°MCLP**: åº”ç”¨åˆ°å…¶ä»–é—®é¢˜
5. **ç”Ÿæˆå¯¹æ¯”ç»“æœ**: å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹

## ğŸ“– ç›¸å…³èµ„æ–™

- [RL4CO Documentation](https://github.com/ai4co/rl4co)
- [Attention, Learn to Solve Routing Problems!](https://arxiv.org/abs/1803.08475)
- [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/)
